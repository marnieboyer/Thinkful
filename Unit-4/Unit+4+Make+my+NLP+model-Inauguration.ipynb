{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "Data cleaning / processing / language parsing\n",
    "Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "Assess your models using cross-validation and determine whether one model performed better.\n",
    "Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "Write up your report in a Jupyter notebook. Be sure to explicitly justify the choices you make throughout, and submit it below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "obama13 = open(\"2013-obama.txt\",encoding=\"utf8\")\n",
    "#print(obama13.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Launch the installer to download \"gutenberg\" and \"stop words\" corpora.\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "nlp = spacy.load('en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?:-[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "\n",
    "inaugural.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a loop\n",
    "#p == 0 \n",
    "#for speech in inaugural.fileids():\n",
    "#    speech = inaugural.raw(speech)\n",
    "#    p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "obama09 = inaugural.raw('2009-Obama.txt')\n",
    "bush05 = inaugural.raw('2005-Bush.txt')\n",
    "bush01 = inaugural.raw('2001-Bush.txt')\n",
    "clinton97 = inaugural.raw('1997-Clinton.txt')\n",
    "clinton93 = inaugural.raw('1993-Clinton.txt')\n",
    "bush89 = inaugural.raw('1989-Bush.txt')\n",
    "reagan85 = inaugural.raw('1985-Reagan.txt')\n",
    "reagan81 = inaugural.raw('1981-Reagan.txt')\n",
    "carter77 = inaugural.raw('1977-Carter.txt')\n",
    "nixon73 = inaugural.raw('1973-Nixon.txt')\n",
    "nixon69 = inaugural.raw('1969-Nixon.txt')\n",
    "johnson65 = inaugural.raw('1965-Johnson.txt')\n",
    "kennedy61 = inaugural.raw('1961-Kennedy.txt')\n",
    "eisenhower57 = inaugural.raw('1957-Eisenhower.txt')\n",
    "eisenhower53= inaugural.raw('1953-Eisenhower.txt')\n",
    "truman49= inaugural.raw('1949-Truman.txt')\n",
    "roosevelt45= inaugural.raw('1945-Roosevelt.txt')\n",
    "roosevelt41= inaugural.raw('1941-Roosevelt.txt')\n",
    "roosevelt37= inaugural.raw('1937-Roosevelt.txt')\n",
    "roosevelt33= inaugural.raw('1933-Roosevelt.txt')\n",
    "\n",
    "\n",
    "lincoln61 = inaugural.raw('1861-Lincoln.txt')\n",
    "lincoln65 = inaugural.raw('1865-Lincoln.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama09 = text_cleaner(obama09)\n",
    "bush05 = text_cleaner(bush05)\n",
    "bush01 = text_cleaner(bush01)\n",
    "clinton97 = text_cleaner(clinton97)\n",
    "clinton93 = text_cleaner(clinton93)\n",
    "bush89 = text_cleaner(bush89)\n",
    "reagan85 = text_cleaner(reagan85)\n",
    "reagan81 = text_cleaner(reagan81)\n",
    "carter77 = text_cleaner(carter77)\n",
    "nixon73 = text_cleaner(nixon73)\n",
    "nixon69 = text_cleaner(nixon69)\n",
    "johnson65 = text_cleaner(johnson65)\n",
    "kennedy61 = text_cleaner(kennedy61)\n",
    "eisenhower57 = text_cleaner(eisenhower57)\n",
    "eisenhower53 = text_cleaner(eisenhower53)\n",
    "truman49 = text_cleaner(truman49)\n",
    "roosevelt45 = text_cleaner(roosevelt45)\n",
    "roosevelt41 = text_cleaner(roosevelt41)\n",
    "roosevelt37 = text_cleaner(roosevelt37)\n",
    "roosevelt33 = text_cleaner(roosevelt33)\n",
    "\n",
    "#obama13 = text_cleaner(obama13)\n",
    "lincoln61 = text_cleaner(lincoln61)\n",
    "lincoln65 = text_cleaner(lincoln65)\n",
    "\n",
    "\n",
    "obama09_doc = nlp(obama09)\n",
    "bush05_doc = nlp(bush05)\n",
    "bush01_doc= nlp(bush01)\n",
    "clinton97_doc = nlp(clinton97)\n",
    "clinton93_doc = nlp(clinton93)\n",
    "bush89_doc = nlp(bush89)\n",
    "reagan85_doc = nlp(reagan85)\n",
    "reagan81_doc = nlp(reagan81)\n",
    "carter77_doc = nlp(carter77)\n",
    "nixon73_doc = nlp(nixon73)\n",
    "nixon69_doc = nlp(nixon69)\n",
    "johnson65_doc = nlp(johnson65)\n",
    "kennedy61_doc = nlp(kennedy61)\n",
    "eisenhower57_doc = nlp(eisenhower57)\n",
    "eisenhower53_doc = nlp(eisenhower53)\n",
    "truman49_doc = nlp(truman49)\n",
    "roosevelt45_doc = nlp(roosevelt45)\n",
    "roosevelt41_doc = nlp(roosevelt41)\n",
    "roosevelt37_doc = nlp(roosevelt37)\n",
    "roosevelt33_doc = nlp(roosevelt33)\n",
    "\n",
    "#obama13_doc = nlp(obama13)\n",
    "lincoln61_doc = nlp(lincoln61)\n",
    "lincoln65_doc = nlp(lincoln65)\n",
    "\n",
    "obama09_sents = [[sent, \"Dem\"] for sent in obama09_doc.sents]\n",
    "bush05_sents = [[sent, \"Rep\"] for sent in bush05_doc.sents]\n",
    "bush01_sents = [[sent, \"Rep\"] for sent in bush01_doc.sents]\n",
    "clinton97_sents = [[sent, \"Dem\"] for sent in clinton97_doc.sents]\n",
    "clinton93_sents = [[sent, \"Dem\"] for sent in clinton93_doc.sents]\n",
    "bush89_sents = [[sent, \"Rep\"] for sent in bush89_doc.sents]\n",
    "reagan85_sents = [[sent, \"Rep\"] for sent in reagan85_doc.sents]\n",
    "reagan81_sents = [[sent, \"Rep\"] for sent in reagan81_doc.sents]\n",
    "carter77_sents = [[sent, \"Dem\"] for sent in carter77_doc.sents]\n",
    "nixon73_sents = [[sent, \"Rep\"] for sent in nixon73_doc.sents]\n",
    "nixon69_sents = [[sent, \"Rep\"] for sent in nixon69_doc.sents]\n",
    "johnson65_sents = [[sent, \"Dem\"] for sent in johnson65_doc.sents]\n",
    "kennedy61_sents = [[sent, \"Dem\"] for sent in kennedy61_doc.sents]\n",
    "eisenhower57_sents = [[sent, \"Rep\"] for sent in eisenhower57_doc.sents]\n",
    "eisenhower53_sents = [[sent, \"Rep\"] for sent in eisenhower53_doc.sents]\n",
    "truman49_sents = [[sent, \"Dem\"] for sent in truman49_doc.sents]\n",
    "roosevelt45_sents = [[sent, \"Dem\"] for sent in roosevelt45_doc.sents]\n",
    "roosevelt41_sents = [[sent, \"Dem\"] for sent in roosevelt41_doc.sents]\n",
    "roosevelt37_sents = [[sent, \"Dem\"] for sent in roosevelt37_doc.sents]\n",
    "roosevelt33_sents = [[sent, \"Dem\"] for sent in roosevelt33_doc.sents]\n",
    "\n",
    "#inform, influence, inspire\n",
    "\n",
    "#obama13_sents = [[sent, \"Obama\"] for sent in obama13_doc.sents]\n",
    "lincoln61_sents = [[sent, \"Lincoln\"] for sent in lincoln61_doc.sents]\n",
    "lincoln65_sents = [[sent, \"Lincoln\"] for sent in lincoln65_doc.sents]\n",
    "\n",
    "\n",
    "sentences = pd.DataFrame(obama09_sents+bush05_sents+bush01_sents+clinton97_sents+clinton93_sents+bush89_sents+\n",
    "                         reagan85_sents + reagan81_sents + carter77_sents + nixon73_sents+ nixon69_sents+johnson65_sents\n",
    "                         +kennedy61_sents+eisenhower57_sents+eisenhower53_sents+truman49_sents+roosevelt45_sents+\n",
    "                        roosevelt41_sents+roosevelt37_sents+roosevelt33_sents) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1848, 2)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "obama09words = bag_of_words(obama09_doc)\n",
    "bush05words = bag_of_words(bush05_doc)\n",
    "bush01words= bag_of_words(bush01_doc)\n",
    "clinton97words = bag_of_words(clinton97_doc)\n",
    "clinton93words = bag_of_words(clinton93_doc)\n",
    "bush89words = bag_of_words(bush89_doc)\n",
    "reagan85words = bag_of_words(reagan85_doc)\n",
    "reagan81words = bag_of_words(reagan81_doc)\n",
    "carter77words = bag_of_words(carter77_doc)\n",
    "nixon73words = bag_of_words(nixon73_doc)\n",
    "nixon69words = bag_of_words(nixon69_doc)\n",
    "johnson65words = bag_of_words(johnson65_doc)\n",
    "kennedy61words = bag_of_words(kennedy61_doc)\n",
    "#obama13_doc = nlp(obama13)\n",
    "lincoln61words = bag_of_words(lincoln61_doc)\n",
    "lincoln65words = bag_of_words(lincoln65_doc)\n",
    "eisenhower57words = bag_of_words(eisenhower57_doc)\n",
    "eisenhower53words = bag_of_words(eisenhower53_doc)\n",
    "truman49words = bag_of_words(truman49_doc)\n",
    "roosevelt45words = bag_of_words(roosevelt45_doc)\n",
    "roosevelt41words = bag_of_words(roosevelt41_doc)\n",
    "roosevelt37words = bag_of_words(roosevelt37_doc)\n",
    "roosevelt33words = bag_of_words(roosevelt33_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(obama09words+bush05words+bush01words+clinton97words+clinton93words+bush89words+\n",
    "                         reagan85words + reagan81words + carter77words + nixon73words+ nixon69words+johnson65words\n",
    "                   +kennedy61words+eisenhower57words+eisenhower53words+truman49words+roosevelt41words+roosevelt37words+roosevelt33words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3293\n"
     ]
    }
   ],
   "source": [
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hesitation</th>\n",
       "      <th>think</th>\n",
       "      <th>magnificently</th>\n",
       "      <th>exciting</th>\n",
       "      <th>trumpet</th>\n",
       "      <th>countryman</th>\n",
       "      <th>helpless</th>\n",
       "      <th>classroom</th>\n",
       "      <th>shackle</th>\n",
       "      <th>creativity</th>\n",
       "      <th>...</th>\n",
       "      <th>eventual</th>\n",
       "      <th>transform</th>\n",
       "      <th>isolation</th>\n",
       "      <th>harmony</th>\n",
       "      <th>strangle</th>\n",
       "      <th>paddy</th>\n",
       "      <th>journey</th>\n",
       "      <th>thick</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(My, fellow, citizens, :, I, stand, here, toda...</td>\n",
       "      <td>Dem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, thank, President, Bush, for, his, service,...</td>\n",
       "      <td>Dem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Forty, -, four, Americans, have, now, taken, ...</td>\n",
       "      <td>Dem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, words, have, been, spoken, during, risin...</td>\n",
       "      <td>Dem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Yet, ,, every, so, often, the, oath, is, take...</td>\n",
       "      <td>Dem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3295 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  hesitation think magnificently exciting trumpet countryman helpless  \\\n",
       "0          0     0             0        0       0          0        0   \n",
       "1          0     0             0        0       0          0        0   \n",
       "2          0     0             0        0       0          0        0   \n",
       "3          0     0             0        0       0          0        0   \n",
       "4          0     0             0        0       0          0        0   \n",
       "\n",
       "  classroom shackle creativity     ...     eventual transform isolation  \\\n",
       "0         0       0          0     ...            0         0         0   \n",
       "1         0       0          0     ...            0         0         0   \n",
       "2         0       0          0     ...            0         0         0   \n",
       "3         0       0          0     ...            0         0         0   \n",
       "4         0       0          0     ...            0         0         0   \n",
       "\n",
       "  harmony strangle paddy journey thick  \\\n",
       "0       0        0     0       0     0   \n",
       "1       0        0     0       0     0   \n",
       "2       0        0     0       0     0   \n",
       "3       0        0     0       0     0   \n",
       "4       0        0     0       0     0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (My, fellow, citizens, :, I, stand, here, toda...         Dem  \n",
       "1  (I, thank, President, Bush, for, his, service,...         Dem  \n",
       "2  (Forty, -, four, Americans, have, now, taken, ...         Dem  \n",
       "3  (The, words, have, been, spoken, during, risin...         Dem  \n",
       "4  (Yet, ,, every, so, often, the, oath, is, take...         Dem  \n",
       "\n",
       "[5 rows x 3295 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trying out BOW [Random Forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.97619047619\n",
      "\n",
      "Test set score: 0.534632034632\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1386, 3293) (1386,)\n",
      "Training set score: 0.943722943723\n",
      "\n",
      "Test set score: 0.597402597403\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Dem</th>\n",
       "      <th>Rep</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dem</th>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rep</th>\n",
       "      <td>98</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0        Dem  Rep\n",
       "text_source          \n",
       "Dem          126   88\n",
       "Rep           98  150"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "lr_predicted = lr.predict(X_test)\n",
    "pd.crosstab(y_test, lr_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.778499278499\n",
      "\n",
      "Test set score: 0.5670995671\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data, this time in the form of paragraphs\n",
    "\n",
    "obama09p= inaugural.paras('2009-Obama.txt')\n",
    "bush05p = inaugural.paras('2005-Bush.txt')\n",
    "bush01p = inaugural.paras('2001-Bush.txt')\n",
    "clinton97p = inaugural.paras('1997-Clinton.txt')\n",
    "clinton93p = inaugural.paras('1993-Clinton.txt')\n",
    "bush89p = inaugural.paras('1989-Bush.txt')\n",
    "reagan85p = inaugural.paras('1985-Reagan.txt')\n",
    "reagan81p = inaugural.paras('1981-Reagan.txt')\n",
    "carter77p = inaugural.paras('1977-Carter.txt')\n",
    "nixon73p = inaugural.paras('1973-Nixon.txt')\n",
    "nixon69p = inaugural.paras('1969-Nixon.txt')\n",
    "johnson65p = inaugural.paras('1965-Johnson.txt')\n",
    "kennedy61p = inaugural.paras('1961-Kennedy.txt')\n",
    "eisenhower57p = inaugural.paras('1957-Eisenhower.txt')\n",
    "eisenhower53p = inaugural.paras('1953-Eisenhower.txt')\n",
    "truman49p = inaugural.paras('1949-Truman.txt')\n",
    "roosevelt45p = inaugural.paras('1945-Roosevelt.txt')\n",
    "roosevelt41p = inaugural.paras('1941-Roosevelt.txt')\n",
    "roosevelt37p = inaugural.paras('1937-Roosevelt.txt')\n",
    "roosevelt33p = inaugural.paras('1933-Roosevelt.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dems1 = obama09p+clinton93p+johnson65p+truman49p+roosevelt41p+roosevelt37p\n",
    "reps1 = bush05p+bush89p+reagan85p+nixon73p+eisenhower53p\n",
    "\n",
    "dems2 =clinton97p+carter77p+kennedy61p+roosevelt45p+roosevelt33p\n",
    "reps2 = bush01p+reagan81p+eisenhower57p+nixon69p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736\n"
     ]
    }
   ],
   "source": [
    "#processing\n",
    "X= []\n",
    "Y = []\n",
    "#pd.DataFrame(index=index, columns=columns)\n",
    "for paragraph in dems1:\n",
    "    para=paragraph[0]\n",
    "    X.append(' '.join(para))\n",
    "    Y.append('Dem')\n",
    "\n",
    "for paragraph in reps1:\n",
    "    para=paragraph[0]\n",
    "    X.append(' '.join(para))\n",
    "    Y.append('Rep')\n",
    "\n",
    "for paragraph in dems2:\n",
    "    para=paragraph[0]\n",
    "    X.append(' '.join(para))\n",
    "    Y.append('Dem')\n",
    "\n",
    "for paragraph in reps2:\n",
    "    para=paragraph[0]\n",
    "    X.append(' '.join(para))\n",
    "    Y.append('Rep')\n",
    "    \n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make df\n",
    "#data = pd.DataFrame({'para': X,'party':Y})\n",
    "# make 2 lists:\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552.0\n"
     ]
    }
   ],
   "source": [
    "h = len(X)*.75\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#75:25 training test split\n",
    "\n",
    "Y_train,Y_test=Y[:552],Y[552:]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.4, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=1, # only use words that appear at least twice\n",
    "                             #analyzer = 'word',\n",
    "                             #ngram_range=(1,3) ,\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#count_vectorizer = CountVectorizer()\n",
    "#count_vectorizer.fit_transform(X)\n",
    "vectorizer.fit_transform(X)\n",
    "\n",
    "#freq_term_matrix = count_vectorizer.transform(X)\n",
    "\n",
    "freq_term_matrix = vectorizer.transform(X)\n",
    "\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(freq_term_matrix)\n",
    "tf_idf_matrix = tfidf.transform(freq_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33152173913043476"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train logistic regression model\n",
    "X_train,X_test=tf_idf_matrix[:552],tf_idf_matrix[552:]\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train,Y_train)\n",
    "pred=logreg.predict(X_test)\n",
    "accuracy_score(Y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0380434782609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialize the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,Y_train)\n",
    "nb_pred=clf.predict(X_test)\n",
    "print(accuracy_score(Y_test, nb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.092391304347826081"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "rf.fit(X_train,Y_train)\n",
    "rf_pred=rf.predict(X_test)\n",
    "accuracy_score(Y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data already loaded as lists of sentences in X and Y\n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(X):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "model = doc2vec.Doc2Vec(docs, size = 160, window = 10, min_count = 7, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making training and test sets\n",
    "wb_Y_train,wb_Y_test=Y_train,Y_test\n",
    "wb_X=[]\n",
    "for i in range(len(X)):\n",
    "    wb_X.append(model.docvecs[i])\n",
    "wb_X_train=wb_X[:552]\n",
    "wb_X_test=wb_X[552:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30978260869565216"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_logreg = linear_model.LogisticRegression(C=1e4)\n",
    "wb_logreg.fit(wb_X_train,wb_Y_train)\n",
    "wb_pred=wb_logreg.predict(wb_X_test)\n",
    "accuracy_score(wb_Y_test, wb_pred)\n",
    "#pd.crosstab(wb_Y_test, wb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22282608695652173"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_clf = GaussianNB()\n",
    "wb_clf.fit(wb_X_train,wb_Y_train)\n",
    "wb_nb_pred=wb_clf.predict(wb_X_test)\n",
    "accuracy_score(wb_Y_test, wb_nb_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
